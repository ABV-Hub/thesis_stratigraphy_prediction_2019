{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages and data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "data_dir = \"/home/ec2-user/pwp-summer-2019/master_thesis_nhh_2019/processed_data/\" \n",
    "history_dir = '/home/ec2-user/SageMaker/LSTM/History/'\n",
    "model_dir = '/home/ec2-user/SageMaker/LSTM/Models/'\n",
    "fig_dir = '/home/ec2-user/SageMaker/OverleafMasterThesis/Images/'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Dense, Dropout, Input, Embedding, \n",
    "                          Dropout, Conv1D, MaxPooling1D, \n",
    "                          BatchNormalization)\n",
    "from keras import callbacks\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.initializers import Zeros\n",
    "\n",
    "#from sklearn import svm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "pd.set_option('display.max_columns', 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Functions import (setup_lstm_stratify, feature_engineering, DataGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle(data_dir+'df_train')\n",
    "df_val = pd.read_pickle(data_dir+'df_val')\n",
    "df_test = pd.read_pickle(data_dir+'df_test')\n",
    "\n",
    "formation_dictionary = joblib.load(data_dir+'formation_dictionary.pkl')\n",
    "n_formation = len(formation_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering and remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_features = {\n",
    "    'outlier_values': {'gr': df_train.append(df_val).gr.quantile(0.9995),\n",
    "                       'rmed': df_train.append(df_val).rmed.quantile(0.9995),\n",
    "                       'rdep': df_train.append(df_val).rdep.quantile(0.9995)\n",
    "                      },\n",
    "    'above_below_variables': ['gr','rdep','rmed'],\n",
    "    'y_variable': 'formation_2',\n",
    "    'num_shifts': 1,\n",
    "    'cols_to_remove' : ['depth', 'dts','hgr', 'hnphi', \n",
    "                        'hrdep', 'hrhob', 'hrmed', 'hrsh','rsh','field','main_area','md'],\n",
    "    'thresh': 7,\n",
    "    'log_variables': ['rmed','rdep'],\n",
    "    'var1_ratio': 'gr'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class = feature_engineering(df_train,**params_features)\n",
    "\n",
    "train_class.remove_outliers()\n",
    "train_class.cleaning()\n",
    "train_class.xyz()\n",
    "train_class.df = train_class.df.dropna(thresh=int(len(train_class.df)*0.9),axis=1)\n",
    "\n",
    "df_train = train_class.df\n",
    "columns_class = df_train.columns\n",
    "\n",
    "val_class = feature_engineering(df_val,**params_features)\n",
    "\n",
    "val_class.remove_outliers()\n",
    "val_class.cleaning()\n",
    "val_class.xyz()\n",
    "df_val = val_class.df[columns_class]\n",
    "\n",
    "print('Control: ', df_val.shape[1] == df_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_keys = ['tvd','gr','x','y','z','rmed','rdep']\n",
    "not_norm_keys = ['title','formation','formation_2','group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = MinMaxScaler().fit(df_train.append(df_val)[norm_keys])\n",
    "scaler = StandardScaler().fit(df_train.append(df_val)[norm_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_scaled = pd.DataFrame(scaler.transform(df_train[norm_keys]),index=df_train.index)\n",
    "\n",
    "df_train_scaled.columns = norm_keys\n",
    "\n",
    "df_train = pd.concat([df_train_scaled,df_train[not_norm_keys]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_scaled = pd.DataFrame(scaler.transform(df_val[norm_keys]),index=df_val.index)\n",
    "\n",
    "df_val_scaled.columns = norm_keys\n",
    "\n",
    "df_val = pd.concat([df_val_scaled,df_val[not_norm_keys]],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove columns with NaNs above threshold/fill NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.fillna(0)\n",
    "df_val = df_val.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_var = 'formation_2'\n",
    "wvars = [column for column in df_train.columns if column not in [y_var,'title','formation','group']]\n",
    "print(wvars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(\n",
    "    df_train,\n",
    "    df_val,\n",
    "    wvars,\n",
    "    batch_size,\n",
    "    win,\n",
    "    n_formation,\n",
    "    callback_patience = 10,\n",
    "    epochs = 100,\n",
    "    generator = True, \n",
    "    seed = 42,\n",
    "    verbose = 1\n",
    "):\n",
    "    \n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(LSTM(50,\n",
    "                    batch_input_shape=(batch_size, len(wvars), win),\n",
    "                    #return_sequences=True, # implement if stacked LSTM layers\n",
    "                    stateful=True, \n",
    "                    kernel_initializer=Zeros()))\n",
    "    \n",
    "    lstm_model.add(Dropout(0.1))\n",
    "    lstm_model.add(Dense(n_formation, activation='softmax')) \n",
    "    \n",
    "    lstm_model.compile(loss='categorical_crossentropy', \n",
    "                   optimizer= Adam(0.01),   \n",
    "                   metrics=['accuracy']) \n",
    "    \n",
    "    early_stopping_cb = callbacks.EarlyStopping(patience=callback_patience,\n",
    "                                                restore_best_weights=True)\n",
    "    # Train\n",
    "    x_train, y_train = setup_lstm_stratify(\n",
    "            df = df_train,\n",
    "            n_val = n_formation,\n",
    "            y_var = y_var,\n",
    "            wvars = wvars,\n",
    "            batch_size = batch_size,\n",
    "            win = win\n",
    "        )\n",
    "    # Validation\n",
    "    x_val, y_val = setup_lstm_stratify(\n",
    "            df = df_val,\n",
    "            n_val = n_formation,\n",
    "            y_var = y_var,\n",
    "            wvars = wvars,\n",
    "            batch_size = batch_size,\n",
    "            win = win\n",
    "        )\n",
    "    \n",
    "    if generator:\n",
    "        training_generator = DataGenerator(x_train, y_train, batch_size)\n",
    "        validation_generator = DataGenerator(x_val, y_val, batch_size)\n",
    "        \n",
    "        history = lstm_model.fit_generator(\n",
    "            generator = training_generator,\n",
    "            epochs = epochs,\n",
    "            validation_data = validation_generator,\n",
    "            shuffle = False,\n",
    "            callbacks = [early_stopping_cb],\n",
    "            verbose = verbose,\n",
    "            use_multiprocessing=True\n",
    "        )\n",
    "        lstm_model.save(model_dir+'lstm_model_generator.h5')\n",
    "        return history\n",
    "    else:\n",
    "        history = lstm_model.fit(x_train,y_train,\n",
    "                         epochs=epochs, \n",
    "                         batch_size=batch_size, \n",
    "                         validation_data=(x_val, y_val),\n",
    "                         verbose=verbose,\n",
    "                         callbacks=[early_stopping_cb]\n",
    "                        )\n",
    "        lstm_model.save(model_dir+'lstm_model.h5')\n",
    "        return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "win = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = lstm_model(\n",
    "    df_train = df_train,\n",
    "    df_val = df_val,\n",
    "    batch_size = batch_size,\n",
    "    win = win,\n",
    "    n_formation = n_formation,\n",
    "    wvars = wvars,\n",
    "    callback_patience = 10,\n",
    "    epochs = 30,\n",
    "    generator = False,\n",
    "    seed = 42,\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blind wells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_pickle(data_dir+'df_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data and set up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_class = feature_engineering(df_test,**params_features)\n",
    "test_class.thresh = 0 # In order to not remove any rows when cleaning\n",
    "test_class.cleaning()\n",
    "test_class.xyz()\n",
    "\n",
    "df_test = test_class.df[columns_class]\n",
    "not_norm_key_test = not_norm_keys.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_scaled = pd.DataFrame(scaler.transform(df_test[norm_keys]),index=df_test.index)\n",
    "\n",
    "df_test_scaled.columns = norm_keys\n",
    "\n",
    "df_test = pd.concat([df_test_scaled,df_test[not_norm_key_test]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = setup_lstm_stratify(\n",
    "    df = df_test,\n",
    "    n_val = n_formation,\n",
    "    y_var = y_var,\n",
    "    wvars = wvars,\n",
    "    batch_size = batch_size,\n",
    "    win = win\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model_loaded = load_model(model_dir+'lstm_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on blind wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_lstm = lstm_model_loaded.predict(x_test, batch_size=batch_size)\n",
    "\n",
    "test_set = pd.DataFrame(pd.DataFrame(y_test).idxmax(axis=1), columns = ['formation_2'])\n",
    "test_set['predicted'] = pd.DataFrame(prediction_lstm).idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If generator function is used:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lstm_model_generator= load_model(model_dir+'lstm_model_generator.h5')\n",
    "\n",
    "test_generator = DataGenerator(x_test, y_test, batch_size)\n",
    "\n",
    "prediction_lstm = lstm_model_generator.predict_generator(test_generator)\n",
    "\n",
    "test_set = pd.DataFrame(pd.DataFrame(y_test).idxmax(axis=1), columns = ['formation_2'])\n",
    "test_set[\"predicted\"] = pd.DataFrame(prediction_lstm).idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.f1_score(test_set[\"formation_2\"], test_set[\"predicted\"],average = 'micro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
