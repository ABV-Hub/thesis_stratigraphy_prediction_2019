{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "data_dir = \"/home/ec2-user/pwp-summer-2019/master_thesis_nhh_2019/processed_data/\" \n",
    "raw_dir = \"/home/ec2-user/pwp-summer-2019/master_thesis_nhh_2019/raw_data/\" \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "pd.set_option('display.max_columns', 999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for splitting the data sets based on formation-distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by: https://stackoverflow.com/questions/56872664/complex-dataset-split-stratifiedgroupshufflesplit\n",
    "\n",
    "def StratifiedGroupShuffleSplit(\n",
    "    df_main,\n",
    "    train_proportion=0.6,\n",
    "    val_proportion = 0.3,\n",
    "    hparam_mse_wgt = 0.1,\n",
    "    df_group=\"title\",\n",
    "    y_var=\"formation_2\",\n",
    "    norm_keys=['gr','tvd','rdep'],\n",
    "    seed = 42\n",
    "):\n",
    "    np.random.seed(seed) # Set seed\n",
    "    df_main.index = range(len(df_main)) # Create unique index for each observation in order to reindex\n",
    "    df_main = df_main.reindex(np.random.permutation(df_main.index)) # Shuffle dataset\n",
    "\n",
    "    # Create empty train, val and test datasets\n",
    "    df_train = pd.DataFrame()\n",
    "    df_val = pd.DataFrame()\n",
    "    df_test = pd.DataFrame()\n",
    "\n",
    "    hparam_mse_wgt = hparam_mse_wgt # Must be between 0 and 1\n",
    "    assert(0 <= hparam_mse_wgt <= 1)\n",
    "    train_proportion = train_proportion # Must be between 0 and 1\n",
    "    assert(0 <= train_proportion <= 1)\n",
    "    val_proportion = val_proportion # Must be between 0 and 1\n",
    "    assert(0 <= val_proportion <= 1)\n",
    "    test_proportion = 1-train_proportion-val_proportion # Remaining in test proportion\n",
    "    assert(0 <= test_proportion <= 1)\n",
    "    \n",
    "    # Group the data set\n",
    "    subject_grouped_df_main = df_main.groupby([df_group], sort=False, as_index=False) \n",
    "    # Find the proportion of the total for each category\n",
    "    category_grouped_df_main = df_main.groupby(y_var).count()[[df_group]]/len(df_main)*100 \n",
    "\n",
    "    # Functoin for calculating MSE\n",
    "    def calc_mse_loss(df):\n",
    "        # Find the proportion of the total for each category in the specific data set\n",
    "        grouped_df = df.groupby(y_var).count()[[df_group]]/len(df)*100\n",
    "        # Merge the data set above with the original proportion for each category\n",
    "        df_temp = category_grouped_df_main.join(grouped_df, on = y_var, how = 'left', lsuffix = '_main')\n",
    "        # Fill NA\n",
    "        df_temp.fillna(0, inplace=True)\n",
    "        # Square the difference\n",
    "        df_temp['diff'] = (df_temp[df_group+'_main'] - df_temp[df_group])**2\n",
    "        # Mean of the squared difference\n",
    "        mse_loss = np.mean(df_temp['diff'])\n",
    "        return mse_loss\n",
    "\n",
    "    # Initialize the train/val/test set\n",
    "    # First three wells are assigned to train/val/test\n",
    "    i = 0\n",
    "    for well, group in subject_grouped_df_main:\n",
    "        group = group.sort_index()\n",
    "        if (i < 3):\n",
    "            if (i == 0):\n",
    "                df_train = df_train.append(pd.DataFrame(group), ignore_index=True)\n",
    "                i += 1\n",
    "                continue\n",
    "            elif (i == 1):\n",
    "                df_val = df_val.append(pd.DataFrame(group), ignore_index=True)\n",
    "                i += 1\n",
    "                continue\n",
    "            else:\n",
    "                df_test = df_test.append(pd.DataFrame(group), ignore_index=True)\n",
    "                i += 1\n",
    "                continue\n",
    "        \n",
    "        # Caluclate the difference between previous dataset and the one in the loop\n",
    "        mse_loss_diff_train = calc_mse_loss(df_train) - calc_mse_loss(df_train.append(pd.DataFrame(group), \n",
    "                                                                                      ignore_index=True))\n",
    "        mse_loss_diff_val = calc_mse_loss(df_val) - calc_mse_loss(df_val.append(pd.DataFrame(group), \n",
    "                                                                                ignore_index=True))\n",
    "        mse_loss_diff_test = calc_mse_loss(df_test) - calc_mse_loss(df_test.append(pd.DataFrame(group), \n",
    "                                                                                   ignore_index=True))\n",
    "        \n",
    "        # Calculate the total lenght so far\n",
    "        total_records = df_train.title.nunique() + df_val.title.nunique() + df_test.title.nunique()\n",
    "\n",
    "        # Calculate how far much much is left before the goal is reached\n",
    "        len_diff_train = (train_proportion - (df_train.title.nunique()/total_records))\n",
    "        len_diff_val = (val_proportion - (df_val.title.nunique()/total_records))\n",
    "        len_diff_test = (test_proportion - (df_test.title.nunique()/total_records))\n",
    "\n",
    "        len_loss_diff_train = len_diff_train * abs(len_diff_train)\n",
    "        len_loss_diff_val = len_diff_val * abs(len_diff_val)\n",
    "        len_loss_diff_test = len_diff_test * abs(len_diff_test)\n",
    "\n",
    "        loss_train = (hparam_mse_wgt * mse_loss_diff_train) + ((1-hparam_mse_wgt) * len_loss_diff_train)\n",
    "        loss_val = (hparam_mse_wgt * mse_loss_diff_val) + ((1-hparam_mse_wgt) * len_loss_diff_val)\n",
    "        loss_test = (hparam_mse_wgt * mse_loss_diff_test) + ((1-hparam_mse_wgt) * len_loss_diff_test)\n",
    "\n",
    "        # Assign to either train, val or test\n",
    "        if (max(loss_train,loss_val,loss_test) == loss_train):\n",
    "            df_train = df_train.append(pd.DataFrame(group), ignore_index=True)\n",
    "        elif (max(loss_train,loss_val,loss_test) == loss_val):\n",
    "            df_val = df_val.append(pd.DataFrame(group), ignore_index=True)\n",
    "        else:\n",
    "            df_test = df_test.append(pd.DataFrame(group), ignore_index=True)\n",
    "\n",
    "        i += 1\n",
    "    \n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for setting up the LSTM data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by: \n",
    "# https://github.com/blasscoc/LinkedInArticles/blob/master/WellFaciesLSTM/LSTM%20Facies%20Competition.ipynb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def chunk(x, y, num_chunks, size=61, random=True):\n",
    "    rng = x.shape[0] - size\n",
    "    if random:\n",
    "        indx = np.int_(\n",
    "            np.random.rand(num_chunks) * rng) + size//2\n",
    "    else:\n",
    "        indx = np.arange(0,rng,1) + size//2\n",
    "    Xwords = np.array([[x[i-size//2:i+size//2+1,:] \n",
    "                                for i in indx]])\n",
    "    ylabel = np.array([y[i] for i in indx])\n",
    "    \n",
    "    return Xwords[0,...], ylabel\n",
    "\n",
    "def _num_pad(size, batch_size):\n",
    "    return (batch_size - np.mod(size, batch_size))\n",
    "\n",
    "def setup_lstm_stratify(df,\n",
    "               df_group='title',\n",
    "               batch_size=128,\n",
    "               wvars=['gr','tvd','rdep'],\n",
    "               y_var = 'formation',\n",
    "               win=9,\n",
    "               n_val=39\n",
    "              ):\n",
    "\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    df_grouped = df.groupby([df_group], sort=False, as_index=False) \n",
    "    df_x = []\n",
    "    df_y = []\n",
    "\n",
    "    for key,val in df_grouped:\n",
    "        val = val.copy() \n",
    "            \n",
    "        _x = val[wvars].values\n",
    "        _y = val[y_var].values\n",
    "        \n",
    "        __x, __y = chunk(_x, _y, 400, size=win, random=False)\n",
    "        \n",
    "        df_x.extend(__x)\n",
    "        df_y.extend(__y)\n",
    "\n",
    "    df_x = np.array(df_x)\n",
    "    df_y = np.array(df_y)    \n",
    "    \n",
    "    #One Hot Encoding\n",
    "    enc = OneHotEncoder(sparse=False, categories=[range(n_val)]) \n",
    "    df_y = enc.fit_transform(np.atleast_2d(df_y).T)\n",
    "    df_x = df_x.transpose(0,2,1)\n",
    "\n",
    "    # pad to batch size    \n",
    "    num_pad = _num_pad(df_x.shape[0], batch_size)\n",
    "    df_x = np.pad(df_x, ((0,num_pad),(0,0),(0,0)), mode='edge')\n",
    "    df_y = np.pad(df_y, ((0,num_pad), (0,0)), mode='edge')\n",
    "        \n",
    "    return df_x, df_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generator for feeding the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, df_x, df_y, batch_size=128):\n",
    "        'Initialization'\n",
    "        self.df_x = df_x\n",
    "        self.df_y = df_y\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.df_x))\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.df_x) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        index_epoch = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        return (self.df_x[index_epoch], self.df_y[index_epoch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for feature engineering and cleaning the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_engineering:\n",
    "    def __init__(self,df, above_below_variables, num_shifts, cols_to_remove,thresh,\n",
    "                 log_variables, y_variable, outlier_values, var1_ratio = 'gr'\n",
    "                ):\n",
    "        #Variables:\n",
    "        self.original_df = df\n",
    "        self.df = df\n",
    "        self.above_below_variables = above_below_variables\n",
    "        self.y_variable = y_variable\n",
    "        self.num_shifts = num_shifts\n",
    "        self.cols_to_remove = cols_to_remove\n",
    "        self.thresh = thresh\n",
    "        self.log_variables = log_variables\n",
    "        self.var1_ratio = var1_ratio\n",
    "        self.outlier_values = outlier_values\n",
    "        self.var2_ratio = log_variables\n",
    "        \n",
    "    def log_values(self):\n",
    "        'Calculates both the log values'\n",
    "        for variable in self.log_variables:\n",
    "            self.df[variable] = np.log(self.df[variable])      \n",
    "        #self.df = self.df.drop(self.log_variables,axis = 1)\n",
    "                      \n",
    "    def above_below(self):\n",
    "        'Add the value above and below for each column in variables'\n",
    "        for var in self.above_below_variables:\n",
    "            self.df[var+'_above'] = self.df.groupby('title')[var].shift(self.num_shifts)\n",
    "            self.df[var+'_below'] = self.df.groupby('title')[var].shift(self.num_shifts)\n",
    "        \n",
    "        drop = [self.above_below_variables[0]+'_above', self.above_below_variables[0]+'_below']\n",
    "        for i in drop:\n",
    "            self.df = self.df.dropna(subset=[i])\n",
    "            \n",
    "    def var_ratio(self):\n",
    "        'Generate the ratio of GR divided by specified variables'\n",
    "        for var in self.var2_ratio:\n",
    "            self.df[self.var1_ratio + '_' + var] = (self.df[self.var1_ratio]/self.df[var])\n",
    "            self.df[self.var1_ratio + '_' + var].loc[self.df[self.var1_ratio + '_' + var] == float('Inf')] = 0\n",
    "            self.df[self.var1_ratio + '_' + var].loc[self.df[self.var1_ratio + '_' + var] == -float('Inf')] = 0\n",
    "        \n",
    "    def cleaning(self):\n",
    "        'Remove certain formations, rows with a lot of NAs and make y_variabl categorical'\n",
    "        self.df = self.df.drop(self.cols_to_remove,axis = 1)\n",
    "        self.df = self.df.dropna(thresh=self.thresh) #thresh= 12\n",
    "\n",
    "        self.df = self.df[np.isfinite(self.df['tvd'])]\n",
    "        self.df = self.df[(self.df.formation != 'water depth')]\n",
    "    \n",
    "        self.df[self.y_variable] = self.df[self.y_variable].astype('category')\n",
    "        self.df = self.df[self.df[self.y_variable].cat.codes != -1]\n",
    "        self.df.reset_index(inplace=True, drop = True)\n",
    "        \n",
    "    def xyz(self):\n",
    "        'Lat/Long for ML purposes'\n",
    "        self.df['x'] = np.cos(self.df['lat']) * np.cos(self.df['long'])\n",
    "        self.df['y'] = np.cos(self.df['lat']) * np.sin(self.df['long'])\n",
    "        self.df['z'] = np.sin(self.df['lat'])\n",
    "        self.df = self.df.drop(['lat','long'],axis = 1)\n",
    "        \n",
    "    def single_pt_haversine(self,degrees=True):\n",
    "        \"\"\"\n",
    "        'Single-point' Haversine: Calculates the great circle distance\n",
    "        between a point on Earth and the (0, 0) lat-long coordinate\n",
    "        \"\"\"\n",
    "        r = 6371 # Earth's radius (km). Have r = 3956 if you want miles\n",
    "\n",
    "        # Convert decimal degrees to radians\n",
    "        if degrees:\n",
    "            lat, lng = map(math.radians, [self.df.lat, self.df.lng])\n",
    "\n",
    "        # 'Single-point' Haversine formula\n",
    "        a = math.sin(lat/2)**2 + math.cos(lat) * math.sin(lng/2)**2\n",
    "        d = 2 * r * math.asin(math.sqrt(a)) \n",
    "        \n",
    "        self.df['well_distance'] = [self.single_pt_haversine(x, y) for x, y in zip(lat, long)]\n",
    "        \n",
    "    def drop_new_values(self):  \n",
    "        'NAs are introduced when we calculate above and below. This function removes them'\n",
    "        drop = [\"gr_above\", \"gr_below\"]\n",
    "        for i in drop:\n",
    "            self.df = self.df.dropna(subset=[i])\n",
    "        self.df = self.df\n",
    "        \n",
    "    def remove_outliers(self):\n",
    "        for key,value in self.outlier_values.items():\n",
    "            self.df = self.df[self.df[key] <= value]\n",
    "            self.df = self.df[self.df[key] >= 0]\n",
    "          \n",
    "    def done(self):\n",
    "        'Return the self.df set and a dictionary of formations and their corresponding number'\n",
    "        self.remove_outliers()\n",
    "        self.remove_outliers()\n",
    "        self.log_values()\n",
    "        self.above_below()\n",
    "        self.cleaning()\n",
    "        self.xyz()\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formation_colors = ['#d96c6c', '#ffe680', '#336633','#4d5766', '#cc99c9', \n",
    "                 '#733939', '#f2eeb6', '#739978', '#333366', '#cc669c', \n",
    "                 '#f2b6b6', '#8a8c69', '#66ccb8', '#bfbfff', '#733950', \n",
    "                 '#b27159', '#c3d96c', '#336663', '#69698c', '#33262b', \n",
    "                 '#bfa38f', '#2d3326', '#1a3133', '#8f66cc', '#99737d', \n",
    "                 '#736256', '#65b359', '#73cfe6', '#673973', '#f2ba79', \n",
    "                 '#bef2b6', '#86aab3', '#554359', '#8c6c46', '#465943', \n",
    "                 '#73b0e6', '#ff80f6', '#4c3b26']\n",
    "group_colors = ['#ff4400', '#cc804e', '#e5b800', '#403300', '#4da63f', \n",
    "                '#133328', '#00cad9', '#005fb3', '#0000f2', '#292259', \n",
    "                '#d052d9', '#33131c', '#ff6176']\n",
    "\n",
    "def plot_well_comparison(df, well_index, formation_colors, group_colors, model_name = None, save = False):\n",
    "    \n",
    "    #df['group_2'] = df['group'].astype('category').cat.codes\n",
    "    \n",
    "    logs = df.loc[df[\"title\"] == df.title.unique()[well_index]]\n",
    "    #logs = logs.sort_values(by='tvd')\n",
    "    \n",
    "    cluster_predicted_formation=np.repeat(np.expand_dims(logs['predicted'].values,1), 100, 1)\n",
    "    \n",
    "    cluster_actual_formation=np.repeat(np.expand_dims(logs['formation_2'].values,1), 100, 1)\n",
    "    \n",
    "    cluster_predicted_group=np.repeat(np.expand_dims(logs['predicted_group'].values,1), 100, 1)\n",
    "    \n",
    "    cluster_actual_group=np.repeat(np.expand_dims(logs['group_2'].values,1), 100, 1)\n",
    "    \n",
    "    cmap_formation = colors.ListedColormap(formation_colors)  \n",
    "    bounds_formation = [l for l in range(n_formation+1)]\n",
    "    norm_formation = colors.BoundaryNorm(bounds_formation, cmap_formation.N)\n",
    "    \n",
    "    cmap_group = colors.ListedColormap(group_colors)\n",
    "    bounds_group = [l for l in range(n_group+1)]\n",
    "    norm_group = colors.BoundaryNorm(bounds_group, cmap_group.N)\n",
    "    \n",
    "    #ztop=logs.tvd.min(); zbot=logs.tvd.max()\n",
    "        \n",
    "    f, ax = plt.subplots(nrows=1, ncols=4, figsize=(8, 12))\n",
    "\n",
    "    im1=ax[0].imshow(cluster_predicted_formation, interpolation='none', aspect='auto',\n",
    "                cmap=cmap_formation,vmin=0,vmax=37)#, norm = norm_formation)\n",
    "    \n",
    "    im2=ax[1].imshow(cluster_actual_formation, interpolation='none', aspect='auto',\n",
    "                cmap=cmap_formation,vmin=0,vmax=37)#, norm = norm_formation)\n",
    "    \n",
    "    im3=ax[2].imshow(cluster_predicted_group, interpolation='none', aspect='auto',\n",
    "                cmap=cmap_group,vmin=0,vmax=12)#, norm = norm_group)\n",
    "    \n",
    "    im4=ax[3].imshow(cluster_actual_group, interpolation='none', aspect='auto',\n",
    "                cmap=cmap_group,vmin=0,vmax=12)#, norm = norm_group)\n",
    "    \n",
    "    ax[0].set_xlabel('Predicted formations')\n",
    "    ax[1].set_xlabel('Actual formations')\n",
    "    ax[2].set_xlabel('Predicted groups')\n",
    "    ax[3].set_xlabel('Actual groups')\n",
    "    \n",
    "    ax[0].set_yticklabels([])\n",
    "    ax[1].set_yticklabels([]); ax[2].set_yticklabels([]); ax[3].set_yticklabels([])\n",
    "    \n",
    "    f.suptitle('Well: %s'%logs.iloc[0]['title'], fontsize=14,y=0.91)\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(fig_dir+'prediction_'+model_name+'_'+'well_'+str(well_index)+'.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "def plot_well_logs(df, well_index, formation_colors, save = False):\n",
    "    \n",
    "    df['group_2'] = df['group'].map(group_dictionary)\n",
    "    \n",
    "    logs = df.loc[df[\"title\"] == df.title.unique()[well_index]]\n",
    "    #logs = logs.sort_values(by='tvd')\n",
    "    cmap_formation = colors.ListedColormap(formation_colors)  \n",
    "    cmap_group = colors.ListedColormap(group_colors1)  \n",
    "    \n",
    "    ztop=logs.tvd.min(); zbot=logs.tvd.max()\n",
    "\n",
    "    cluster=np.repeat(np.expand_dims(logs['formation_2'].values,1), 100, 1)\n",
    "    cluster_2=np.repeat(np.expand_dims(logs['group_2'].values,1), 100, 1)\n",
    "    \n",
    "    f, ax = plt.subplots(nrows=1, ncols=8, figsize=(12, 16))\n",
    "    ax[0].plot(logs.gr, logs.tvd, '-g')\n",
    "    ax[1].plot(logs.rdep, logs.tvd, '-')\n",
    "    ax[2].plot(logs.rmed, logs.tvd, '-', color='r')\n",
    "    ax[3].plot(logs.dt, logs.tvd, '-', color='0.5')\n",
    "    ax[4].plot(logs.nphi, logs.tvd, '-', color='y')\n",
    "    ax[5].plot(logs.rhob, logs.tvd, '-', color='c')\n",
    "\n",
    "    im1=ax[6].imshow(cluster, interpolation='none', aspect='auto',\n",
    "                cmap=cmap_formation,vmin=1,vmax=37)\n",
    "    im2=ax[7].imshow(cluster_2, interpolation='none', aspect='auto',\n",
    "                cmap=cmap_group,vmin=1,vmax=12)\n",
    "    \n",
    "    for i in range(len(ax)-2):\n",
    "        ax[i].set_ylim(ztop,zbot)\n",
    "        ax[i].invert_yaxis()\n",
    "        ax[i].grid()\n",
    "        ax[i].locator_params(axis='x', nbins=3)\n",
    "    \n",
    "    ax[0].set_xlabel(\"gr\")\n",
    "    ax[0].set_xlim(logs.gr.min(),logs.gr.max()+10)\n",
    "    ax[1].set_xlabel(\"rdep\")\n",
    "    ax[1].set_xlim(logs.rdep.min(),logs.rdep.max()+0.5)\n",
    "    ax[2].set_xlabel(\"rmed\")\n",
    "    ax[2].set_xlim(logs.rmed.min(),logs.rmed.max()+0.5)\n",
    "    ax[3].set_xlabel(\"dt\")\n",
    "    ax[3].set_xlim(logs.dt.min(),logs.dt.max()+0.5)\n",
    "    ax[4].set_xlabel(\"nphi\")\n",
    "    ax[5].set_xlim(logs.nphi.min(),logs.nphi.max()+0.5)\n",
    "    ax[5].set_xlabel(\"rhob\")\n",
    "    ax[5].set_xlim(logs.rhob.min(),logs.rhob.max()+0.5)\n",
    "    ax[6].set_xlabel('Formations')\n",
    "    ax[7].set_xlabel('Group')\n",
    "    \n",
    "    ax[1].set_yticklabels([]); ax[2].set_yticklabels([]); ax[3].set_yticklabels([]);ax[4].set_yticklabels([])\n",
    "    ax[5].set_yticklabels([]); ax[6].set_yticklabels([]); ax[7].set_yticklabels([])\n",
    "    f.suptitle('Well: %s'%logs.iloc[0]['title'], fontsize=14,y=0.91)\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(fig_dir+'well_'+str(well_index)+'.01.png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
